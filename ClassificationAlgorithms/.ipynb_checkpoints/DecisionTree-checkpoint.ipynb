{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the filename along with relative path: project3_dataset1.txt\n",
      "[['20.13' '28.25' '131.2' ... '0.2572' '0.06637' '1']\n",
      " ['12.45' '15.7' '82.57' ... '0.3985' '0.1244' '1']\n",
      " ['11.26' '19.96' '73.72' ... '0.2955' '0.07009' '0']\n",
      " ...\n",
      " ['14.5' '10.89' '94.28' ... '0.2889' '0.08006' '0']\n",
      " ['12.36' '18.54' '79.01' ... '0.2983' '0.07185' '0']\n",
      " ['11.93' '21.53' '76.53' ... '0.2438' '0.08541' '0']]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "import sys\n",
    "filename = input(\"Enter the filename along with relative path: \")\n",
    "# filename = \"project3_dataset2.txt\"\n",
    "data = []\n",
    "for i in open(filename, 'r').readlines():\n",
    "        data.append(i.rstrip().split())    \n",
    "data = np.array(data);\n",
    "print(data)\n",
    "trainData = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isString(data):\n",
    "    try:\n",
    "        float(data)\n",
    "        return False\n",
    "    except:\n",
    "        pass\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_index(groups, classes):\n",
    "    total_rows = 0.0;\n",
    "    for grp in groups:\n",
    "        total_rows += len(grp)\n",
    "        \n",
    "    gini = 0.0\n",
    "    for group in groups:\n",
    "        size = float(len(group))\n",
    "        if size == 0:\n",
    "            continue\n",
    "        score = 0.0\n",
    "        for val in classes:\n",
    "            count = 0\n",
    "            for row in group:\n",
    "                if(row[-1] == val):\n",
    "                    count = count+1;\n",
    "            proportion = count/size   \n",
    "            score += proportion * proportion\n",
    "        gini += (1.0 - score) * (size / total_rows)\n",
    "    return gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(data, value, ind):\n",
    "    left =[]\n",
    "    right = []\n",
    "    for row in data:\n",
    "        if(row[ind] >= value):\n",
    "            right.append(row)\n",
    "        else:\n",
    "            left.append(row)\n",
    "    return np.array(left), np.array(right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalSplit(data):\n",
    "    unique_classes = np.unique(data[:,-1])\n",
    "    split_Ind, split_Val, Split_Data, minscore = 9999999, 9999999, None, 9999999\n",
    "    #not including the class label\n",
    "    d = {}\n",
    "    for ind in range(len(data[0])-1):\n",
    "        for row in data:\n",
    "            samples =[]\n",
    "            left , right = splitData(data, row[ind], ind)\n",
    "            samples.append(left)\n",
    "            samples.append(right)\n",
    "            giniScore = gini_index(samples, unique_classes)\n",
    "            if(giniScore<minscore):\n",
    "                split_Ind, split_Val, Split_Data, minscore = ind, row[ind], samples, giniScore\n",
    "    d['index'] = split_Ind\n",
    "    d['val'] = split_Val\n",
    "    d['sample'] = Split_Data\n",
    "    d['score'] = minscore\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitNode(node):\n",
    "    left, right = node['sample']\n",
    "    \n",
    "    if((left.size==0) or (right.size==0)):\n",
    "        classes = []\n",
    "        if(left.size==0):\n",
    "            classes = right[:,-1]  \n",
    "        else:\n",
    "            classes = left[:,-1]\n",
    "        count0 = 0;\n",
    "        count1 = 0;\n",
    "        labelCount  = collections.Counter(classes)\n",
    "        top = labelCount.most_common()\n",
    "        node['left'] = node['right'] = top[0][0]\n",
    "        return\n",
    "    \n",
    "    if (len(left)<=1):\n",
    "        classes = left[:,-1]\n",
    "        labelCount  = collections.Counter(classes)\n",
    "        top = labelCount.most_common()\n",
    "        node['left'] = top[0][0]\n",
    "    else:\n",
    "        node['left'] = evalSplit(left)\n",
    "        splitNode(node['left'])\n",
    "    \n",
    "    if(len(right)<=1):\n",
    "        classes = right[:,-1]\n",
    "        labelCount  = collections.Counter(classes)\n",
    "        top = labelCount.most_common()\n",
    "        node['right'] = top[0][0]\n",
    "    else:\n",
    "        node['right'] = evalSplit(right)\n",
    "        splitNode(node['right'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(node, row):\n",
    "    if row[node['index']] < node['val']:\n",
    "        if isinstance(node['left'], dict):\n",
    "            return predict(node['left'], row)\n",
    "        else:\n",
    "            return node['left']\n",
    "    else:\n",
    "        if isinstance(node['right'], dict):\n",
    "            return predict(node['right'], row)\n",
    "        else:\n",
    "            return node['right']\n",
    "\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "def DecisionTreePrediction(traindata, testdata):\n",
    "    predictions = list()\n",
    "    actual = list(testdata[:,-1])\n",
    "    root = evalSplit(traindata)\n",
    "    splitNode(root)\n",
    "    tree = root\n",
    "#     print_tree(tree, 0)\n",
    "    for row in testdata:\n",
    "        predictions.append(predict(tree, row))\n",
    "    return actual,predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(predictedLabels,trueLabels):\n",
    "    a=0\n",
    "    b=0\n",
    "    c=0\n",
    "    d=0\n",
    "    for i, j in zip(trueLabels,predictedLabels):\n",
    "            if ((j == 1) & (i == j)):\n",
    "                a += 1\n",
    "            elif ((j == 0) & (i != j)):\n",
    "                b += 1\n",
    "            elif ((j == 1) & (i != j)):\n",
    "                c += 1\n",
    "            elif ((j == 0) & (i == j)):\n",
    "                d += 1\n",
    "    sum = a+b+c+d\n",
    "    if sum == 0:\n",
    "        accuracy = \"Not possible to compute\"\n",
    "    else:\n",
    "        accuracy = (a+d)/sum\n",
    "    if a+c == 0:\n",
    "        precision = \"Not possible to compute\"\n",
    "    else:\n",
    "        precision = a/(a+c)\n",
    "    if a+b == 0:\n",
    "        recall = \"Not possible to compute\"\n",
    "    else:\n",
    "        recall = a/(a+b)\n",
    "    if a+b+c == 0:\n",
    "        f1 = \"Not possible to compute\"\n",
    "    else:\n",
    "        f1 = 2*a/(2*a+b+c)\n",
    "    return accuracy,precision,recall,f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 0\n",
      "Training data Length: 513\n",
      "Validation data Length: 56\n",
      "accuracy 0.9642857142857143\n",
      "precision 0.9473684210526315\n",
      "recall 0.9473684210526315\n",
      "f1 0.9473684210526315\n",
      "\n",
      "Iteration 1\n",
      "Training data Length: 512\n",
      "Validation data Length: 57\n",
      "accuracy 0.8771929824561403\n",
      "precision 0.8695652173913043\n",
      "recall 0.8333333333333334\n",
      "f1 0.851063829787234\n",
      "\n",
      "Iteration 2\n",
      "Training data Length: 512\n",
      "Validation data Length: 57\n",
      "accuracy 0.8421052631578947\n",
      "precision 0.7619047619047619\n",
      "recall 0.8\n",
      "f1 0.7804878048780488\n"
     ]
    }
   ],
   "source": [
    "randomdata = np.random.permutation(len(data))\n",
    "length = len(randomdata)\n",
    "Allaccuracies = []\n",
    "Allprecision = []\n",
    "Allrecall =[]\n",
    "Allf1score = []\n",
    "length = len(randomdata)\n",
    "for i in range(10):\n",
    "    tdata = []\n",
    "    tdataLabels = []\n",
    "    vdata = []\n",
    "    vdataLabels = []\n",
    "    start = i *length//10\n",
    "    end = (i+1)*length//10\n",
    "    for j in range(length):\n",
    "        if(j in range(start,end)):\n",
    "            vdata.append(data[randomdata[j]])\n",
    "            vdataLabels.append(data[randomdata[j]][-1])\n",
    "        else:\n",
    "            tdata.append(data[randomdata[j]])\n",
    "            tdataLabels.append(data[randomdata[j]][-1])\n",
    "    tdata = np.array(tdata)\n",
    "    vdata = np.array(vdata)\n",
    "#     print(tdata.shape)\n",
    "    actualLabels, predictedLabels  = DecisionTreePrediction(tdata,vdata)\n",
    "    actualLabels = np.array(actualLabels).astype(int)\n",
    "    predictedLabels = np.array(predictedLabels).astype(int)\n",
    "    accuracy,precision,recall,f1 = metrics(predictedLabels,actualLabels)\n",
    "    if not (isString(accuracy)):\n",
    "        Allaccuracies.append(accuracy)\n",
    "    if not (isString(precision)):\n",
    "        Allprecision.append(precision)\n",
    "    if not (isString(recall)):\n",
    "        Allrecall.append(recall)\n",
    "    if not (isString(f1)):\n",
    "        Allf1score.append(f1)\n",
    "    print(\"\\nIteration \" + str(i))\n",
    "    print(\"Training data Length: \" + str(len(tdata)))\n",
    "    print(\"Validation data Length: \" + str(len(vdata)))\n",
    "    print(\"accuracy \" + str(accuracy))\n",
    "    print(\"precision \" + str(precision))\n",
    "    print(\"recall \" + str(recall))\n",
    "    print(\"f1 \" + str(f1))\n",
    "print(\"\\nAverageaccuracy \" + str(np.array(Allaccuracies).mean()))\n",
    "print(\"Averageprecision \" + str(np.array(Allprecision).mean()))\n",
    "print(\"Averagerecall \" + str(np.array(Allrecall).mean()))\n",
    "print(\"Averagef1 \" + str(np.array(Allf1score).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
